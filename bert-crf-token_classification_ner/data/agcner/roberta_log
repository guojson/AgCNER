E:\Anaconda3\envs\torch\python.exe E:\guo\NER\bert-crf-token_classification_ner-master\train.py
num_labels 是30
Downloading: 100%|██████████| 107k/107k [00:00<00:00, 587kB/s]
Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 2.69kB/s]
Downloading: 100%|██████████| 112/112 [00:00<00:00, 112kB/s]
Downloading: 100%|██████████| 19.0/19.0 [00:00<?, ?B/s]
Downloading: 100%|██████████| 689/689 [00:00<00:00, 694kB/s]
E:\guo\NER\bert-crf-token_classification_ner-master\dataset.py:48: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.


  df = pd.read_csv(file, engine='python', encoding=csv_encoding, error_bad_lines=False)
====================  Preparing for training  ====================
	* Loading training data...
数据集个数为47839
输入例子
近年来，稻纵卷叶螟呈暴发趋势，严重影响水稻的生长，给我国农业生产造成巨大损失
attention_mask
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0])
input_ids
tensor([ 101, 6818, 2399, 3341, 8024, 4940, 5288, 1318, 1383,  100, 1439, 3274,
        1355, 6633, 1232, 8024,  698, 7028, 2512, 1510, 3717, 4940, 4638, 4495,
        7270, 8024, 5314, 2769, 1744, 1093,  689, 4495,  772, 6863, 2768, 2342,
        1920, 2938, 1927,  102,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0])
labels
tensor([3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 7, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2])
实际序列转换后的长度为150, 设置最长为150
E:\guo\NER\bert-crf-token_classification_ner-master\dataset.py:48: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.


  df = pd.read_csv(file, engine='python', encoding=csv_encoding, error_bad_lines=False)
数据集个数为5963
输入例子
在这些庇护作物上，棉铃虫能在不接触Bt毒蛋白的条件下繁殖
attention_mask
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0])
input_ids
tensor([ 101, 1762, 6821,  763, 2413, 2844,  868, 4289,  677, 8024, 3469, 7190,
        6001, 5543, 1762,  679, 2970, 6239,  100,  162, 3681, 6028, 4635, 4638,
        3340,  816,  678, 5246, 3658,  102,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0])
labels
tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2])
实际序列转换后的长度为150, 设置最长为150
E:\guo\NER\bert-crf-token_classification_ner-master\dataset.py:48: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.


  df = pd.read_csv(file, engine='python', encoding=csv_encoding, error_bad_lines=False)
数据集个数为5974
输入例子
一般先从植株下部叶片开始发病，少数从中部叶片开始发生逐步向上部叶片蔓延
attention_mask
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0])
input_ids
tensor([ 101,  671, 5663, 1044,  794, 3490, 3415,  678, 6956, 1383, 4275, 2458,
        1993, 1355, 4567, 8024, 2208, 3144,  794,  704, 6956, 1383, 4275, 2458,
        1993, 1355, 4495, 6852, 3635, 1403,  677, 6956, 1383, 4275, 5913, 2454,
         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0])
labels
tensor([ 3,  3,  3,  3,  3,  3,  3,  3,  3, 12, 13,  3,  3,  3,  3,  3,  3,  3,
         3,  3,  3, 12, 13,  3,  3,  3,  3,  3,  3,  3,  3,  3, 12, 13,  3,  3,
         3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,
         2,  2,  2,  2,  2,  2])
实际序列转换后的长度为150, 设置最长为150
	* Loading validation data...
	* Building model...
You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.
Downloading: 100%|██████████| 393M/393M [00:46<00:00, 8.94MB/s]
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing RobertaModel: ['bert.encoder.layer.4.attention.self.query.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
E:\Anaconda3\envs\torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
正在对验证集进行测试
E:\guo\NER\bert-crf-token_classification_ner-master\model.py:246: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorCompare.cpp:402.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
{'acc': 2.3201586988550018e-05, 'recall': 9.865824782951855e-05, 'f1': 3.756820978088342e-05, 'num': 20272}
{'DIS': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 2704}, 'PIR': {'acc': 0, 'recall': 0.0, 'f1': 0.0, 'num': 2098}, 'PIT': {'acc': 0.0001, 'recall': 0.0004, 'f1': 0.0002, 'num': 4493}, 'CRO': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 4239}, 'STRAINS': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 1093}, 'BIL': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 195}, 'PART': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 1711}, 'ORG': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 313}, 'DRUG': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 2615}, 'COI': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 64}, 'CLA': {'acc': 0, 'recall': 0.0, 'f1': 0.0, 'num': 232}, 'RIA': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 438}, 'FIR': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 77}}
before train-> Valid. time: 18.9405s, loss: 220.0370, f1_score: 0.0038%


 ==================== Training model on device: cuda:0 ====================
* Training epoch 1:
loss:12.789510726928711: 100%|██████████| 1495/1495 [08:25<00:00,  2.95it/s]
-> Training time: 505.9510s, loss = 19.5540, accuracy:
* Validation for epoch 1:
正在对验证集进行测试
{'acc': 0.7404520217598565, 'recall': 0.6512924230465666, 'f1': 0.6930162979292969, 'num': 20272}
{'DIS': {'acc': 0.6675, 'recall': 0.8672, 'f1': 0.7544, 'num': 2704}, 'CRO': {'acc': 0.8185, 'recall': 0.6947, 'f1': 0.7516, 'num': 4239}, 'FIR': {'acc': 0.3636, 'recall': 0.2597, 'f1': 0.303, 'num': 77}, 'RIA': {'acc': 0.7045, 'recall': 0.1416, 'f1': 0.2357, 'num': 438}, 'PIR': {'acc': 0.7132, 'recall': 0.5915, 'f1': 0.6467, 'num': 2098}, 'DRUG': {'acc': 0.635, 'recall': 0.6099, 'f1': 0.6222, 'num': 2615}, 'PART': {'acc': 0.7824, 'recall': 0.4582, 'f1': 0.578, 'num': 1711}, 'PIT': {'acc': 0.8577, 'recall': 0.7663, 'f1': 0.8095, 'num': 4493}, 'CLA': {'acc': 0.0, 'recall': 0.0, 'f1': 0.0, 'num': 232}, 'STRAINS': {'acc': 0.6068, 'recall': 0.5224, 'f1': 0.5615, 'num': 1093}, 'BIL': {'acc': 0.6337, 'recall': 0.559, 'f1': 0.594, 'num': 195}, 'COI': {'acc': 0.5417, 'recall': 0.4062, 'f1': 0.4643, 'num': 64}, 'ORG': {'acc': 0.4306, 'recall': 0.1981, 'f1': 0.2713, 'num': 313}}
-> Valid. time: 30.5857s, loss: 8.8517, f1_score: 69.3016%

save model
* Training epoch 2:
loss:4.969451904296875: 100%|██████████| 1495/1495 [09:21<00:00,  2.66it/s]
-> Training time: 561.6286s, loss = 6.2943, accuracy:
* Validation for epoch 2:
正在对验证集进行测试
{'acc': 0.794031771168407, 'recall': 0.7914857932123125, 'f1': 0.792756738061711, 'num': 20272}
{'CRO': {'acc': 0.8194, 'recall': 0.9014, 'f1': 0.8585, 'num': 4239}, 'STRAINS': {'acc': 0.8083, 'recall': 0.6212, 'f1': 0.7025, 'num': 1093}, 'ORG': {'acc': 0.5744, 'recall': 0.3578, 'f1': 0.4409, 'num': 313}, 'PIR': {'acc': 0.7517, 'recall': 0.7316, 'f1': 0.7415, 'num': 2098}, 'PART': {'acc': 0.6791, 'recall': 0.7347, 'f1': 0.7058, 'num': 1711}, 'DRUG': {'acc': 0.7312, 'recall': 0.6792, 'f1': 0.7042, 'num': 2615}, 'PIT': {'acc': 0.838, 'recall': 0.8694, 'f1': 0.8534, 'num': 4493}, 'DIS': {'acc': 0.8861, 'recall': 0.9175, 'f1': 0.9015, 'num': 2704}, 'BIL': {'acc': 0.8093, 'recall': 0.8051, 'f1': 0.8072, 'num': 195}, 'CLA': {'acc': 0.3214, 'recall': 0.0388, 'f1': 0.0692, 'num': 232}, 'RIA': {'acc': 0.7065, 'recall': 0.4726, 'f1': 0.5663, 'num': 438}, 'COI': {'acc': 0.4694, 'recall': 0.7188, 'f1': 0.5679, 'num': 64}, 'FIR': {'acc': 0.5221, 'recall': 0.7662, 'f1': 0.6211, 'num': 77}}
-> Valid. time: 30.0232s, loss: 5.0779, f1_score: 79.2757%

save model
* Training epoch 3:
loss:2.7651588916778564: 100%|██████████| 1495/1495 [09:26<00:00,  2.64it/s]
-> Training time: 566.9779s, loss = 4.0095, accuracy:
* Validation for epoch 3:
正在对验证集进行测试
{'acc': 0.8584592306868103, 'recall': 0.7904498816101027, 'f1': 0.8230520314345884, 'num': 20272}
{'PIT': {'acc': 0.9056, 'recall': 0.882, 'f1': 0.8937, 'num': 4493}, 'CRO': {'acc': 0.9017, 'recall': 0.8745, 'f1': 0.8879, 'num': 4239}, 'PIR': {'acc': 0.7641, 'recall': 0.7412, 'f1': 0.7525, 'num': 2098}, 'PART': {'acc': 0.8075, 'recall': 0.6569, 'f1': 0.7245, 'num': 1711}, 'DRUG': {'acc': 0.8063, 'recall': 0.7006, 'f1': 0.7497, 'num': 2615}, 'STRAINS': {'acc': 0.837, 'recall': 0.6203, 'f1': 0.7126, 'num': 1093}, 'COI': {'acc': 0.47, 'recall': 0.7344, 'f1': 0.5732, 'num': 64}, 'DIS': {'acc': 0.9223, 'recall': 0.909, 'f1': 0.9156, 'num': 2704}, 'CLA': {'acc': 0.3208, 'recall': 0.0733, 'f1': 0.1193, 'num': 232}, 'FIR': {'acc': 0.6512, 'recall': 0.7273, 'f1': 0.6871, 'num': 77}, 'RIA': {'acc': 0.7902, 'recall': 0.6963, 'f1': 0.7403, 'num': 438}, 'ORG': {'acc': 0.6615, 'recall': 0.4058, 'f1': 0.503, 'num': 313}, 'BIL': {'acc': 0.8245, 'recall': 0.7949, 'f1': 0.8094, 'num': 195}}
-> Valid. time: 30.0611s, loss: 3.6527, f1_score: 82.3052%

save model
* Training epoch 4:
loss:2.394676446914673: 100%|██████████| 1495/1495 [09:25<00:00,  2.64it/s]
-> Training time: 565.2232s, loss = 3.1994, accuracy:
* Validation for epoch 4:
正在对验证集进行测试
{'acc': 0.8510310073533193, 'recall': 0.816397000789266, 'f1': 0.8333543140562452, 'num': 20272}
{'DRUG': {'acc': 0.81, 'recall': 0.7289, 'f1': 0.7673, 'num': 2615}, 'DIS': {'acc': 0.9308, 'recall': 0.9149, 'f1': 0.9228, 'num': 2704}, 'CRO': {'acc': 0.9045, 'recall': 0.8889, 'f1': 0.8966, 'num': 4239}, 'RIA': {'acc': 0.6506, 'recall': 0.7991, 'f1': 0.7172, 'num': 438}, 'PART': {'acc': 0.7638, 'recall': 0.7259, 'f1': 0.7444, 'num': 1711}, 'PIT': {'acc': 0.8974, 'recall': 0.897, 'f1': 0.8972, 'num': 4493}, 'PIR': {'acc': 0.7947, 'recall': 0.7641, 'f1': 0.7791, 'num': 2098}, 'ORG': {'acc': 0.557, 'recall': 0.5463, 'f1': 0.5516, 'num': 313}, 'FIR': {'acc': 0.6923, 'recall': 0.8182, 'f1': 0.75, 'num': 77}, 'STRAINS': {'acc': 0.8412, 'recall': 0.6542, 'f1': 0.736, 'num': 1093}, 'CLA': {'acc': 0.3143, 'recall': 0.0948, 'f1': 0.1457, 'num': 232}, 'COI': {'acc': 0.7167, 'recall': 0.6719, 'f1': 0.6935, 'num': 64}, 'BIL': {'acc': 0.7409, 'recall': 0.8359, 'f1': 0.7855, 'num': 195}}
-> Valid. time: 30.2503s, loss: 3.2871, f1_score: 83.3354%

save model
* Training epoch 5:
loss:2.904494524002075: 100%|██████████| 1495/1495 [09:26<00:00,  2.64it/s]
-> Training time: 566.4492s, loss = 2.6779, accuracy:
* Validation for epoch 5:
正在对验证集进行测试
{'acc': 0.8483810866609598, 'recall': 0.8310970797158642, 'f1': 0.8396501457725948, 'num': 20272}
{'CRO': {'acc': 0.91, 'recall': 0.8969, 'f1': 0.9034, 'num': 4239}, 'PIR': {'acc': 0.8172, 'recall': 0.7908, 'f1': 0.8038, 'num': 2098}, 'DRUG': {'acc': 0.8032, 'recall': 0.7598, 'f1': 0.7809, 'num': 2615}, 'PART': {'acc': 0.8393, 'recall': 0.6867, 'f1': 0.7554, 'num': 1711}, 'DIS': {'acc': 0.8999, 'recall': 0.9342, 'f1': 0.9167, 'num': 2704}, 'PIT': {'acc': 0.9009, 'recall': 0.9065, 'f1': 0.9037, 'num': 4493}, 'RIA': {'acc': 0.5527, 'recall': 0.8379, 'f1': 0.6661, 'num': 438}, 'FIR': {'acc': 0.7105, 'recall': 0.7013, 'f1': 0.7059, 'num': 77}, 'STRAINS': {'acc': 0.7854, 'recall': 0.7301, 'f1': 0.7568, 'num': 1093}, 'CLA': {'acc': 0.2787, 'recall': 0.2198, 'f1': 0.2458, 'num': 232}, 'BIL': {'acc': 0.8368, 'recall': 0.8154, 'f1': 0.826, 'num': 195}, 'ORG': {'acc': 0.5976, 'recall': 0.4792, 'f1': 0.5319, 'num': 313}, 'COI': {'acc': 0.6812, 'recall': 0.7344, 'f1': 0.7068, 'num': 64}}
-> Valid. time: 29.9709s, loss: 3.1133, f1_score: 83.9650%

save model
* Training epoch 6:
loss:2.2330737113952637: 100%|██████████| 1495/1495 [09:29<00:00,  2.62it/s]
-> Training time: 569.6358s, loss = 2.2873, accuracy:
* Validation for epoch 6:
正在对验证集进行测试
{'acc': 0.8500024941387739, 'recall': 0.840568271507498, 'f1': 0.8452590590044398, 'num': 20272}
{'DRUG': {'acc': 0.7595, 'recall': 0.7813, 'f1': 0.7702, 'num': 2615}, 'PIT': {'acc': 0.9066, 'recall': 0.9143, 'f1': 0.9105, 'num': 4493}, 'ORG': {'acc': 0.5546, 'recall': 0.6006, 'f1': 0.5767, 'num': 313}, 'CRO': {'acc': 0.9014, 'recall': 0.9054, 'f1': 0.9034, 'num': 4239}, 'PIR': {'acc': 0.8372, 'recall': 0.7645, 'f1': 0.7992, 'num': 2098}, 'DIS': {'acc': 0.9032, 'recall': 0.9382, 'f1': 0.9204, 'num': 2704}, 'BIL': {'acc': 0.798, 'recall': 0.8103, 'f1': 0.8041, 'num': 195}, 'PART': {'acc': 0.8129, 'recall': 0.7463, 'f1': 0.7782, 'num': 1711}, 'FIR': {'acc': 0.7294, 'recall': 0.8052, 'f1': 0.7654, 'num': 77}, 'RIA': {'acc': 0.8298, 'recall': 0.7237, 'f1': 0.7732, 'num': 438}, 'CLA': {'acc': 0.3492, 'recall': 0.2845, 'f1': 0.3135, 'num': 232}, 'STRAINS': {'acc': 0.7875, 'recall': 0.7356, 'f1': 0.7606, 'num': 1093}, 'COI': {'acc': 0.6552, 'recall': 0.5938, 'f1': 0.623, 'num': 64}}
-> Valid. time: 30.6011s, loss: 3.0038, f1_score: 84.5259%

save model
* Training epoch 7:
loss:2.9841744899749756: 100%|██████████| 1495/1495 [09:33<00:00,  2.61it/s]
-> Training time: 573.5528s, loss = 1.9513, accuracy:
* Validation for epoch 7:
正在对验证集进行测试
{'acc': 0.8645922638559685, 'recall': 0.845698500394633, 'f1': 0.8550410214209122, 'num': 20272}
{'PIR': {'acc': 0.8249, 'recall': 0.7679, 'f1': 0.7954, 'num': 2098}, 'DRUG': {'acc': 0.8156, 'recall': 0.7816, 'f1': 0.7983, 'num': 2615}, 'DIS': {'acc': 0.9419, 'recall': 0.9345, 'f1': 0.9382, 'num': 2704}, 'PIT': {'acc': 0.9239, 'recall': 0.9083, 'f1': 0.916, 'num': 4493}, 'CRO': {'acc': 0.8983, 'recall': 0.9396, 'f1': 0.9185, 'num': 4239}, 'STRAINS': {'acc': 0.8208, 'recall': 0.6789, 'f1': 0.7431, 'num': 1093}, 'COI': {'acc': 0.8049, 'recall': 0.5156, 'f1': 0.6286, 'num': 64}, 'ORG': {'acc': 0.5068, 'recall': 0.5974, 'f1': 0.5484, 'num': 313}, 'RIA': {'acc': 0.7412, 'recall': 0.8174, 'f1': 0.7774, 'num': 438}, 'PART': {'acc': 0.8164, 'recall': 0.7668, 'f1': 0.7908, 'num': 1711}, 'BIL': {'acc': 0.8424, 'recall': 0.7949, 'f1': 0.8179, 'num': 195}, 'CLA': {'acc': 0.3288, 'recall': 0.2069, 'f1': 0.254, 'num': 232}, 'FIR': {'acc': 0.6176, 'recall': 0.8182, 'f1': 0.7039, 'num': 77}}
-> Valid. time: 30.8126s, loss: 2.8441, f1_score: 85.5041%

save model
* Training epoch 8:
loss:1.7525216341018677: 100%|██████████| 1495/1495 [09:25<00:00,  2.64it/s]
-> Training time: 565.2528s, loss = 1.6523, accuracy:
* Validation for epoch 8:
正在对验证集进行测试
{'acc': 0.8495007830853563, 'recall': 0.8562056037884768, 'f1': 0.8528400157232704, 'num': 20272}
{'PIT': {'acc': 0.9197, 'recall': 0.9072, 'f1': 0.9134, 'num': 4493}, 'DIS': {'acc': 0.9283, 'recall': 0.9427, 'f1': 0.9354, 'num': 2704}, 'PIR': {'acc': 0.7599, 'recall': 0.8251, 'f1': 0.7911, 'num': 2098}, 'STRAINS': {'acc': 0.8133, 'recall': 0.7173, 'f1': 0.7623, 'num': 1093}, 'ORG': {'acc': 0.6194, 'recall': 0.5304, 'f1': 0.5714, 'num': 313}, 'CRO': {'acc': 0.8842, 'recall': 0.9387, 'f1': 0.9106, 'num': 4239}, 'DRUG': {'acc': 0.8169, 'recall': 0.7897, 'f1': 0.803, 'num': 2615}, 'PART': {'acc': 0.7512, 'recall': 0.7978, 'f1': 0.7738, 'num': 1711}, 'BIL': {'acc': 0.7477, 'recall': 0.8205, 'f1': 0.7824, 'num': 195}, 'CLA': {'acc': 0.4643, 'recall': 0.1121, 'f1': 0.1806, 'num': 232}, 'COI': {'acc': 0.6571, 'recall': 0.7188, 'f1': 0.6866, 'num': 64}, 'RIA': {'acc': 0.7419, 'recall': 0.7877, 'f1': 0.7641, 'num': 438}, 'FIR': {'acc': 0.6915, 'recall': 0.8442, 'f1': 0.7602, 'num': 77}}
-> Valid. time: 30.0506s, loss: 2.9712, f1_score: 85.2840%

* Training epoch 9:
loss:1.251287817955017: 100%|██████████| 1495/1495 [09:31<00:00,  2.62it/s]
-> Training time: 571.0880s, loss = 1.3087, accuracy:
* Validation for epoch 9:
正在对验证集进行测试
{'acc': 0.8512613619792933, 'recall': 0.8639009471191792, 'f1': 0.8575345819561758, 'num': 20272}
{'DRUG': {'acc': 0.7934, 'recall': 0.8138, 'f1': 0.8035, 'num': 2615}, 'PART': {'acc': 0.8125, 'recall': 0.7878, 'f1': 0.8, 'num': 1711}, 'STRAINS': {'acc': 0.7946, 'recall': 0.7575, 'f1': 0.7756, 'num': 1093}, 'PIT': {'acc': 0.889, 'recall': 0.9268, 'f1': 0.9075, 'num': 4493}, 'COI': {'acc': 0.5843, 'recall': 0.8125, 'f1': 0.6797, 'num': 64}, 'CRO': {'acc': 0.9101, 'recall': 0.9224, 'f1': 0.9162, 'num': 4239}, 'DIS': {'acc': 0.9291, 'recall': 0.9401, 'f1': 0.9346, 'num': 2704}, 'PIR': {'acc': 0.768, 'recall': 0.8251, 'f1': 0.7955, 'num': 2098}, 'RIA': {'acc': 0.8019, 'recall': 0.7854, 'f1': 0.7935, 'num': 438}, 'CLA': {'acc': 0.5422, 'recall': 0.194, 'f1': 0.2857, 'num': 232}, 'BIL': {'acc': 0.8457, 'recall': 0.8154, 'f1': 0.8303, 'num': 195}, 'FIR': {'acc': 0.7065, 'recall': 0.8442, 'f1': 0.7692, 'num': 77}, 'ORG': {'acc': 0.5811, 'recall': 0.6294, 'f1': 0.6043, 'num': 313}}
-> Valid. time: 29.9713s, loss: 3.1081, f1_score: 85.7535%

save model
* Training epoch 10:
loss:1.0796862840652466: 100%|██████████| 1495/1495 [09:25<00:00,  2.64it/s]
-> Training time: 565.5503s, loss = 1.0925, accuracy:
* Validation for epoch 10:
正在对验证集进行测试
{'acc': 0.8635057471264368, 'recall': 0.8597573007103394, 'f1': 0.8616274471030256, 'num': 20272}
{'CRO': {'acc': 0.9086, 'recall': 0.9288, 'f1': 0.9186, 'num': 4239}, 'PIT': {'acc': 0.9327, 'recall': 0.9128, 'f1': 0.9226, 'num': 4493}, 'DRUG': {'acc': 0.8185, 'recall': 0.8073, 'f1': 0.8129, 'num': 2615}, 'CLA': {'acc': 0.3966, 'recall': 0.306, 'f1': 0.3455, 'num': 232}, 'PIR': {'acc': 0.786, 'recall': 0.8017, 'f1': 0.7938, 'num': 2098}, 'DIS': {'acc': 0.9306, 'recall': 0.9471, 'f1': 0.9388, 'num': 2704}, 'STRAINS': {'acc': 0.797, 'recall': 0.7402, 'f1': 0.7676, 'num': 1093}, 'RIA': {'acc': 0.7797, 'recall': 0.8082, 'f1': 0.7937, 'num': 438}, 'PART': {'acc': 0.8061, 'recall': 0.7896, 'f1': 0.7978, 'num': 1711}, 'ORG': {'acc': 0.6232, 'recall': 0.5655, 'f1': 0.593, 'num': 313}, 'COI': {'acc': 0.6, 'recall': 0.6562, 'f1': 0.6269, 'num': 64}, 'BIL': {'acc': 0.8029, 'recall': 0.8564, 'f1': 0.8288, 'num': 195}, 'FIR': {'acc': 0.6804, 'recall': 0.8571, 'f1': 0.7586, 'num': 77}}
-> Valid. time: 30.1285s, loss: 2.9646, f1_score: 86.1627%

save model
* Training epoch 11:
loss:0.7934821248054504: 100%|██████████| 1495/1495 [09:26<00:00,  2.64it/s]
-> Training time: 566.4835s, loss = 0.9260, accuracy:
* Validation for epoch 11:
正在对验证集进行测试
{'acc': 0.873649505452701, 'recall': 0.8496448303078137, 'f1': 0.8614799809938231, 'num': 20272}
{'DIS': {'acc': 0.9235, 'recall': 0.946, 'f1': 0.9346, 'num': 2704}, 'CRO': {'acc': 0.9033, 'recall': 0.9339, 'f1': 0.9183, 'num': 4239}, 'PIT': {'acc': 0.9256, 'recall': 0.9083, 'f1': 0.9169, 'num': 4493}, 'PIR': {'acc': 0.8465, 'recall': 0.7622, 'f1': 0.8021, 'num': 2098}, 'PART': {'acc': 0.8325, 'recall': 0.7668, 'f1': 0.7983, 'num': 1711}, 'DRUG': {'acc': 0.8077, 'recall': 0.8046, 'f1': 0.8061, 'num': 2615}, 'CLA': {'acc': 0.5217, 'recall': 0.2069, 'f1': 0.2963, 'num': 232}, 'STRAINS': {'acc': 0.8118, 'recall': 0.7457, 'f1': 0.7773, 'num': 1093}, 'COI': {'acc': 0.6212, 'recall': 0.6406, 'f1': 0.6308, 'num': 64}, 'ORG': {'acc': 0.6208, 'recall': 0.5335, 'f1': 0.5739, 'num': 313}, 'FIR': {'acc': 0.7532, 'recall': 0.7532, 'f1': 0.7532, 'num': 77}, 'BIL': {'acc': 0.8049, 'recall': 0.8462, 'f1': 0.825, 'num': 195}, 'RIA': {'acc': 0.8568, 'recall': 0.7237, 'f1': 0.7847, 'num': 438}}
-> Valid. time: 30.1467s, loss: 3.0460, f1_score: 86.1480%

E:\guo\NER\bert-crf-token_classification_ner-master\utils.py:724: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure()
* Training epoch 12:
loss:0.7529663443565369: 100%|██████████| 1495/1495 [09:29<00:00,  2.63it/s]
-> Training time: 569.3981s, loss = 0.7160, accuracy:
* Validation for epoch 12:
正在对验证集进行测试
{'acc': 0.8499588596873336, 'recall': 0.8662687450670876, 'f1': 0.858036303227225, 'num': 20272}
{'PIT': {'acc': 0.9128, 'recall': 0.9201, 'f1': 0.9164, 'num': 4493}, 'DIS': {'acc': 0.9401, 'recall': 0.9397, 'f1': 0.9399, 'num': 2704}, 'DRUG': {'acc': 0.8154, 'recall': 0.8141, 'f1': 0.8148, 'num': 2615}, 'CRO': {'acc': 0.917, 'recall': 0.9144, 'f1': 0.9157, 'num': 4239}, 'PART': {'acc': 0.7996, 'recall': 0.7908, 'f1': 0.7952, 'num': 1711}, 'PIR': {'acc': 0.7556, 'recall': 0.8475, 'f1': 0.7989, 'num': 2098}, 'STRAINS': {'acc': 0.7524, 'recall': 0.7813, 'f1': 0.7666, 'num': 1093}, 'RIA': {'acc': 0.703, 'recall': 0.8539, 'f1': 0.7711, 'num': 438}, 'ORG': {'acc': 0.6214, 'recall': 0.5559, 'f1': 0.5868, 'num': 313}, 'CLA': {'acc': 0.3376, 'recall': 0.3405, 'f1': 0.3391, 'num': 232}, 'COI': {'acc': 0.6026, 'recall': 0.7344, 'f1': 0.662, 'num': 64}, 'BIL': {'acc': 0.801, 'recall': 0.8256, 'f1': 0.8131, 'num': 195}, 'FIR': {'acc': 0.7093, 'recall': 0.7922, 'f1': 0.7485, 'num': 77}}
-> Valid. time: 29.9072s, loss: 3.4488, f1_score: 85.8036%

* Training epoch 13:
loss:0.7177463173866272: 100%|██████████| 1495/1495 [09:25<00:00,  2.64it/s]
-> Training time: 565.8053s, loss = 0.5478, accuracy:
* Validation for epoch 13:
正在对验证集进行测试
{'acc': 0.871690734055355, 'recall': 0.85758681925809, 'f1': 0.8645812611895763, 'num': 20272}
{'CRO': {'acc': 0.9044, 'recall': 0.9351, 'f1': 0.9195, 'num': 4239}, 'PIR': {'acc': 0.8403, 'recall': 0.785, 'f1': 0.8117, 'num': 2098}, 'DIS': {'acc': 0.9532, 'recall': 0.9416, 'f1': 0.9473, 'num': 2704}, 'DRUG': {'acc': 0.8217, 'recall': 0.8141, 'f1': 0.8179, 'num': 2615}, 'STRAINS': {'acc': 0.7447, 'recall': 0.7713, 'f1': 0.7578, 'num': 1093}, 'ORG': {'acc': 0.6128, 'recall': 0.4601, 'f1': 0.5255, 'num': 313}, 'PIT': {'acc': 0.9326, 'recall': 0.9145, 'f1': 0.9235, 'num': 4493}, 'PART': {'acc': 0.8113, 'recall': 0.7814, 'f1': 0.7961, 'num': 1711}, 'CLA': {'acc': 0.4, 'recall': 0.2155, 'f1': 0.2801, 'num': 232}, 'RIA': {'acc': 0.7701, 'recall': 0.8105, 'f1': 0.7898, 'num': 438}, 'FIR': {'acc': 0.7722, 'recall': 0.7922, 'f1': 0.7821, 'num': 77}, 'BIL': {'acc': 0.8394, 'recall': 0.8308, 'f1': 0.8351, 'num': 195}, 'COI': {'acc': 0.6333, 'recall': 0.5938, 'f1': 0.6129, 'num': 64}}
-> Valid. time: 30.2492s, loss: 3.6015, f1_score: 86.4581%

save model
* Training epoch 14:
loss:0.3982451558113098: 100%|██████████| 1495/1495 [09:29<00:00,  2.63it/s]
-> Training time: 569.4886s, loss = 0.4612, accuracy:
* Validation for epoch 14:
正在对验证集进行测试
{'acc': 0.8475506960165695, 'recall': 0.8679952644041041, 'f1': 0.8576511588233859, 'num': 20272}
{'PIT': {'acc': 0.9068, 'recall': 0.9266, 'f1': 0.9166, 'num': 4493}, 'CRO': {'acc': 0.903, 'recall': 0.9335, 'f1': 0.918, 'num': 4239}, 'DIS': {'acc': 0.9432, 'recall': 0.946, 'f1': 0.9446, 'num': 2704}, 'ORG': {'acc': 0.585, 'recall': 0.5495, 'f1': 0.5667, 'num': 313}, 'DRUG': {'acc': 0.7933, 'recall': 0.7985, 'f1': 0.7959, 'num': 2615}, 'PIR': {'acc': 0.7784, 'recall': 0.8389, 'f1': 0.8075, 'num': 2098}, 'PART': {'acc': 0.771, 'recall': 0.8147, 'f1': 0.7923, 'num': 1711}, 'STRAINS': {'acc': 0.7697, 'recall': 0.7521, 'f1': 0.7608, 'num': 1093}, 'RIA': {'acc': 0.8153, 'recall': 0.7763, 'f1': 0.7953, 'num': 438}, 'BIL': {'acc': 0.8241, 'recall': 0.841, 'f1': 0.8325, 'num': 195}, 'FIR': {'acc': 0.6465, 'recall': 0.8312, 'f1': 0.7273, 'num': 77}, 'CLA': {'acc': 0.3224, 'recall': 0.3405, 'f1': 0.3312, 'num': 232}, 'COI': {'acc': 0.6604, 'recall': 0.5469, 'f1': 0.5983, 'num': 64}}
-> Valid. time: 29.9320s, loss: 3.7569, f1_score: 85.7651%

* Training epoch 15:
loss:0.4103838801383972: 100%|██████████| 1495/1495 [09:23<00:00,  2.65it/s]
-> Training time: 563.1864s, loss = 0.3677, accuracy:
* Validation for epoch 15:
正在对验证集进行测试
{'acc': 0.8594793713163065, 'recall': 0.8632103393843725, 'f1': 0.8613408151210867, 'num': 20272}
{'PIT': {'acc': 0.9286, 'recall': 0.9152, 'f1': 0.9219, 'num': 4493}, 'CRO': {'acc': 0.9041, 'recall': 0.9358, 'f1': 0.9197, 'num': 4239}, 'PIR': {'acc': 0.7897, 'recall': 0.8074, 'f1': 0.7985, 'num': 2098}, 'PART': {'acc': 0.8069, 'recall': 0.789, 'f1': 0.7979, 'num': 1711}, 'STRAINS': {'acc': 0.7244, 'recall': 0.7768, 'f1': 0.7497, 'num': 1093}, 'DIS': {'acc': 0.9421, 'recall': 0.9501, 'f1': 0.9461, 'num': 2704}, 'DRUG': {'acc': 0.8269, 'recall': 0.8038, 'f1': 0.8152, 'num': 2615}, 'ORG': {'acc': 0.5719, 'recall': 0.5974, 'f1': 0.5844, 'num': 313}, 'COI': {'acc': 0.6522, 'recall': 0.4688, 'f1': 0.5455, 'num': 64}, 'RIA': {'acc': 0.7458, 'recall': 0.8037, 'f1': 0.7736, 'num': 438}, 'FIR': {'acc': 0.7317, 'recall': 0.7792, 'f1': 0.7547, 'num': 77}, 'CLA': {'acc': 0.438, 'recall': 0.2586, 'f1': 0.3252, 'num': 232}, 'BIL': {'acc': 0.7557, 'recall': 0.8564, 'f1': 0.8029, 'num': 195}}
-> Valid. time: 30.2151s, loss: 3.9728, f1_score: 86.1341%

* Training epoch 16:
loss:0.3772867023944855: 100%|██████████| 1495/1495 [09:22<00:00,  2.66it/s]
-> Training time: 562.5980s, loss = 0.2880, accuracy:
* Validation for epoch 16:
正在对验证集进行测试
{'acc': 0.8606182729640733, 'recall': 0.8638022888713497, 'f1': 0.8622073413919594, 'num': 20272}
{'CRO': {'acc': 0.9165, 'recall': 0.9247, 'f1': 0.9206, 'num': 4239}, 'STRAINS': {'acc': 0.7737, 'recall': 0.7539, 'f1': 0.7637, 'num': 1093}, 'PIT': {'acc': 0.9109, 'recall': 0.9263, 'f1': 0.9186, 'num': 4493}, 'PIR': {'acc': 0.7993, 'recall': 0.8236, 'f1': 0.8113, 'num': 2098}, 'CLA': {'acc': 0.4357, 'recall': 0.2629, 'f1': 0.328, 'num': 232}, 'DRUG': {'acc': 0.8307, 'recall': 0.7973, 'f1': 0.8137, 'num': 2615}, 'DIS': {'acc': 0.9372, 'recall': 0.9438, 'f1': 0.9405, 'num': 2704}, 'PART': {'acc': 0.7937, 'recall': 0.789, 'f1': 0.7913, 'num': 1711}, 'RIA': {'acc': 0.7202, 'recall': 0.8402, 'f1': 0.7756, 'num': 438}, 'COI': {'acc': 0.6724, 'recall': 0.6094, 'f1': 0.6393, 'num': 64}, 'ORG': {'acc': 0.5662, 'recall': 0.6422, 'f1': 0.6018, 'num': 313}, 'BIL': {'acc': 0.8626, 'recall': 0.8051, 'f1': 0.8329, 'num': 195}, 'FIR': {'acc': 0.6809, 'recall': 0.8312, 'f1': 0.7485, 'num': 77}}
-> Valid. time: 30.1032s, loss: 4.1904, f1_score: 86.2207%

* Training epoch 17:
loss:0.135765939950943: 100%|██████████| 1495/1495 [09:23<00:00,  2.66it/s]
-> Training time: 563.0116s, loss = 0.2296, accuracy:
* Validation for epoch 17:
正在对验证集进行测试
{'acc': 0.867260579064588, 'recall': 0.8643942383583267, 'f1': 0.8658250364404476, 'num': 20272}
{'CRO': {'acc': 0.9247, 'recall': 0.9181, 'f1': 0.9214, 'num': 4239}, 'PIR': {'acc': 0.807, 'recall': 0.8194, 'f1': 0.8132, 'num': 2098}, 'DIS': {'acc': 0.9476, 'recall': 0.949, 'f1': 0.9483, 'num': 2704}, 'PIT': {'acc': 0.9159, 'recall': 0.9237, 'f1': 0.9198, 'num': 4493}, 'DRUG': {'acc': 0.8317, 'recall': 0.8069, 'f1': 0.8191, 'num': 2615}, 'PART': {'acc': 0.7942, 'recall': 0.796, 'f1': 0.7951, 'num': 1711}, 'FIR': {'acc': 0.6703, 'recall': 0.7922, 'f1': 0.7262, 'num': 77}, 'RIA': {'acc': 0.796, 'recall': 0.8196, 'f1': 0.8076, 'num': 438}, 'STRAINS': {'acc': 0.7799, 'recall': 0.7749, 'f1': 0.7774, 'num': 1093}, 'ORG': {'acc': 0.5352, 'recall': 0.607, 'f1': 0.5689, 'num': 313}, 'COI': {'acc': 0.7115, 'recall': 0.5781, 'f1': 0.6379, 'num': 64}, 'BIL': {'acc': 0.8077, 'recall': 0.8615, 'f1': 0.8337, 'num': 195}, 'CLA': {'acc': 0.4697, 'recall': 0.2672, 'f1': 0.3407, 'num': 232}}
-> Valid. time: 30.2347s, loss: 4.0531, f1_score: 86.5825%

save model
* Training epoch 18:
loss:0.09800129383802414: 100%|██████████| 1495/1495 [09:22<00:00,  2.66it/s]
-> Training time: 562.6594s, loss = 0.1936, accuracy:
* Validation for epoch 18:
正在对验证集进行测试
{'acc': 0.8730857771994796, 'recall': 0.8605958958168903, 'f1': 0.8667958463755154, 'num': 20272}
{'CRO': {'acc': 0.9236, 'recall': 0.9217, 'f1': 0.9227, 'num': 4239}, 'DRUG': {'acc': 0.8403, 'recall': 0.795, 'f1': 0.8171, 'num': 2615}, 'DIS': {'acc': 0.9421, 'recall': 0.9516, 'f1': 0.9468, 'num': 2704}, 'RIA': {'acc': 0.8431, 'recall': 0.7854, 'f1': 0.8132, 'num': 438}, 'PIT': {'acc': 0.9249, 'recall': 0.9234, 'f1': 0.9242, 'num': 4493}, 'PIR': {'acc': 0.8266, 'recall': 0.8155, 'f1': 0.821, 'num': 2098}, 'ORG': {'acc': 0.5921, 'recall': 0.524, 'f1': 0.5559, 'num': 313}, 'PART': {'acc': 0.788, 'recall': 0.7972, 'f1': 0.7926, 'num': 1711}, 'CLA': {'acc': 0.3957, 'recall': 0.2371, 'f1': 0.2965, 'num': 232}, 'STRAINS': {'acc': 0.7682, 'recall': 0.7612, 'f1': 0.7647, 'num': 1093}, 'COI': {'acc': 0.6441, 'recall': 0.5938, 'f1': 0.6179, 'num': 64}, 'BIL': {'acc': 0.83, 'recall': 0.8513, 'f1': 0.8405, 'num': 195}, 'FIR': {'acc': 0.6809, 'recall': 0.8312, 'f1': 0.7485, 'num': 77}}
-> Valid. time: 30.2286s, loss: 4.2531, f1_score: 86.6796%

save model
* Training epoch 19:
loss:0.30861225724220276: 100%|██████████| 1495/1495 [09:22<00:00,  2.66it/s]
-> Training time: 562.2140s, loss = 0.1729, accuracy:
* Validation for epoch 19:
正在对验证集进行测试
{'acc': 0.8682220354900895, 'recall': 0.8664660615627466, 'f1': 0.8673431597659432, 'num': 20272}
{'PIR': {'acc': 0.8104, 'recall': 0.8189, 'f1': 0.8146, 'num': 2098}, 'CRO': {'acc': 0.9116, 'recall': 0.9347, 'f1': 0.923, 'num': 4239}, 'DIS': {'acc': 0.9341, 'recall': 0.9538, 'f1': 0.9438, 'num': 2704}, 'DRUG': {'acc': 0.8227, 'recall': 0.8161, 'f1': 0.8194, 'num': 2615}, 'PART': {'acc': 0.8153, 'recall': 0.7972, 'f1': 0.8061, 'num': 1711}, 'PIT': {'acc': 0.9216, 'recall': 0.9261, 'f1': 0.9238, 'num': 4493}, 'BIL': {'acc': 0.8019, 'recall': 0.8513, 'f1': 0.8259, 'num': 195}, 'ORG': {'acc': 0.6288, 'recall': 0.5304, 'f1': 0.5754, 'num': 313}, 'FIR': {'acc': 0.7262, 'recall': 0.7922, 'f1': 0.7578, 'num': 77}, 'STRAINS': {'acc': 0.7681, 'recall': 0.7575, 'f1': 0.7628, 'num': 1093}, 'CLA': {'acc': 0.4538, 'recall': 0.2328, 'f1': 0.3077, 'num': 232}, 'RIA': {'acc': 0.8237, 'recall': 0.7466, 'f1': 0.7832, 'num': 438}, 'COI': {'acc': 0.6164, 'recall': 0.7031, 'f1': 0.6569, 'num': 64}}
-> Valid. time: 30.1438s, loss: 4.4187, f1_score: 86.7343%

save model
  0%|          | 0/1495 [00:00<?, ?it/s]* Training epoch 20:
loss:0.09724210947751999: 100%|██████████| 1495/1495 [09:22<00:00,  2.66it/s]
-> Training time: 562.6196s, loss = 0.1615, accuracy:
* Validation for epoch 20:
正在对验证集进行测试
{'acc': 0.8662908569450626, 'recall': 0.8641969218626677, 'f1': 0.8652426225459933, 'num': 20272}
{'PIT': {'acc': 0.9155, 'recall': 0.923, 'f1': 0.9192, 'num': 4493}, 'CRO': {'acc': 0.9222, 'recall': 0.9283, 'f1': 0.9252, 'num': 4239}, 'PART': {'acc': 0.8106, 'recall': 0.7902, 'f1': 0.8002, 'num': 1711}, 'PIR': {'acc': 0.7693, 'recall': 0.836, 'f1': 0.8013, 'num': 2098}, 'DIS': {'acc': 0.9396, 'recall': 0.9493, 'f1': 0.9444, 'num': 2704}, 'RIA': {'acc': 0.8338, 'recall': 0.7557, 'f1': 0.7928, 'num': 438}, 'CLA': {'acc': 0.4688, 'recall': 0.2586, 'f1': 0.3333, 'num': 232}, 'STRAINS': {'acc': 0.7776, 'recall': 0.7356, 'f1': 0.756, 'num': 1093}, 'DRUG': {'acc': 0.8392, 'recall': 0.8103, 'f1': 0.8245, 'num': 2615}, 'ORG': {'acc': 0.6151, 'recall': 0.5974, 'f1': 0.6062, 'num': 313}, 'COI': {'acc': 0.5775, 'recall': 0.6406, 'f1': 0.6074, 'num': 64}, 'BIL': {'acc': 0.804, 'recall': 0.8205, 'f1': 0.8122, 'num': 195}, 'FIR': {'acc': 0.7045, 'recall': 0.8052, 'f1': 0.7515, 'num': 77}}
-> Valid. time: 30.2693s, loss: 4.5074, f1_score: 86.5243%


Process finished with exit code 0
